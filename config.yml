# BioMotif-PPI Configuration File
# Protein-Protein Interaction Prediction with Motif Discovery and Flash Attention

# Data Configuration
data:
  dataset_name: "Synthyra/bernett_gold_ppi"
  data_dir: "./data"
  cache_dir: "./cache"
  sequence_source: "uniprot"
  uniprot_release: "2023_04"
  
  # Data splits
  train_split: "train"
  val_split: "valid"
  test_split: "test"
  
  # Preprocessing
  min_seq_length: 0
  max_seq_length: 50000
  remove_non_standard_aa: false
  non_standard_aa: ["B", "J", "O", "U", "X", "Z"]
  max_gap_fraction: 1.0  # Allow any amount of gaps
  max_unknown_fraction: 1.0  # Allow any amount of unknown residues
  
  # Batch sizes
  train_batch_size: 32
  val_batch_size: 64
  test_batch_size: 64
  
  # Data loading
  num_workers: 8
  pin_memory: true
  prefetch_factor: 2

# ESM-2 Configuration
esm:
  model_name: "facebook/esm2_t30_150M_UR50D"
  embedding_dim: 640
  batch_size: 32
  max_length: 1022
  truncation_strategy: "center"
  cache_embeddings: true
  cache_format: "hdf5"
  compression: "lzf"
  
# Physicochemical Properties
properties:
  hydrophobicity:
    scale: "KyteDoolittle"
    normalize: true
    range: [-1, 1]
  
  charge:
    ph: 7.0
    values: {"-": -1, "0": 0, "+": 1}
  
  size:
    scale: "Zamyatnin"
    normalize: true
  
  aromaticity:
    residues: ["F", "W", "Y"]
  
  flexibility:
    source: "PDB_BFactor_2023"
  
  solvent_accessibility:
    scale: "Janin"
  
  # Secondary structure
  secondary_structure:
    method: "ChouFasman_corrected"
    window_size: 17
    
# Model Architecture
model:
  # Feature encoding
  encoder:
    esm_projection_dim: 512
    property_projection_dim: 64
    combined_dim: 576
    
  # BiGRU
  bigru:
    hidden_size: 512
    num_layers: 2
    dropout: 0.1
    bidirectional: true
    output_dim: 512
    
  # Motif Discovery
  motif_discovery:
    num_motifs: 128
    motif_dim: 256
    temperature_init: 0.1
    temperature_range: [0.01, 1.0]
    query_projection_dim: 256
    use_flash_attention: true
    flash_block_size: 64
    dropout: 0.1
    
  # Complementarity Analysis
  complementarity:
    chunk_size: 100
    chunk_overlap: 10
    top_k_pairs: 1000
    
    # Complementarity weights (learnable, initialized to 0.2)
    weights:
      hydrophobic: 0.2
      electrostatic: 0.2
      size: 0.2
      aromatic: 0.2
      hbond: 0.2
    
    # Function parameters
    hydrophobic_threshold: 0.3
    electrostatic_decay: 4.0
    size_sigma: 0.5
    aromatic_optimal_distance: 7
    aromatic_distance_tolerance: 3
    
    # Aggregation MLP
    mlp_dims: [256, 128, 64, 32]
    
  # Allosteric GNN
  gnn:
    num_layers: 5
    hidden_dim: 256
    num_heads: 4
    head_dim: 64
    contact_threshold: 0.3
    use_flash_attention: true
    dropout: 0.1
    
    # FFN in each layer
    ffn_dim: 1024
    activation: "gelu"
    
    # Graph construction
    include_sequential_edges: true
    average_degree_target: 15
    max_degree: 50
    
    # Readout
    readout_method: "attention_pool"
    global_pool_dims: [512, 256]
    
  # Final prediction
  prediction:
    # Component MLPs
    direct_mlp_dims: [32, 16, 1]
    motif_mlp_dims: [512, 128, 16, 1]
    allosteric_mlp_dims: [512, 128, 16, 1]
    
    # Ensemble
    ensemble_method: "weighted_sum"
    init_weights: [0.33, 0.33, 0.34]
    
# Training Configuration
training:
  # Optimization
  optimizer:
    type: "AdamW"
    lr: 0.0001  # 1e-4
    betas: [0.9, 0.999]
    weight_decay: 0.00001  # 1e-5
    eps: 0.00000001  # 1e-8
    foreach: true
    
  # Learning rate schedule
  scheduler:
    type: "CosineAnnealingWarmRestarts"
    T_0: 10
    T_mult: 2
    eta_min: 0.000001  # 1e-6
    
  # Training settings
  max_epochs: 30
  early_stopping_patience: 10
  gradient_clip_norm: 1.0
  
  # Mixed precision
  use_amp: true
  amp_dtype: "float16"
  amp_init_scale: 65536  # 2^16
  amp_growth_interval: 2000
  
  # Memory optimization
  gradient_checkpointing: true
  checkpoint_layers: [1, 3]  # Checkpoint every other GNN layer
  clear_cache_interval: 50
  
  # Loss
  loss_function: "BCEWithLogitsLoss"
  pos_weight: 1.0  # Balanced dataset
  
# Validation Configuration
validation:
  # Metrics
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "auc_roc"
    - "auc_pr"
    - "mcc"
  
  # Validation frequency
  val_check_interval: 1.0  # Every epoch
  
  # Best model selection
  monitor_metric: "val_f1"
  monitor_mode: "max"
  
# Flash Attention Configuration
flash_attention:
  enabled: true
  window_size: [-1, -1]  # Full attention
  alibi_slopes: null
  deterministic: false
  softmax_scale: null  # Use default 1/sqrt(d)
  
# Hardware Configuration
hardware:
  device: "cuda"
  gpu_id: 0
  num_gpus: 1
  
  # Memory management
  empty_cache_interval: 50
  max_memory_allocated: 45000  # MB (for L40S with 48GB)
  
  # Benchmarking
  cudnn_benchmark: true
  cudnn_deterministic: false
  
# Logging Configuration
logging:
  # Experiment tracking
  use_wandb: false
  wandb_project: "biomotif-ppi"
  wandb_entity: null  # Use default
  
  use_tensorboard: true
  tensorboard_dir: "./logs/tensorboard"
  
  # Logging frequency
  log_every_n_steps: 50
  log_grad_norm: true
  log_memory_usage: true
  
  # Checkpointing
  checkpoint_dir: "./checkpoints"
  save_top_k: 3
  save_last: true
  checkpoint_frequency: 1  # Every epoch
  
# Reproducibility
reproducibility:
  seed: 42
  deterministic_algorithms: false  # True slows down training
  
# Paths
paths:
  project_root: "."
  data_root: "./data"
  cache_root: "./cache"
  checkpoint_root: "./checkpoints"
  log_root: "./logs"
  results_root: "./results"
  
# Experiment Configuration
experiment:
  name: "biomotif_ppi_v1"
  description: "BioMotif-PPI with Flash Attention"
  tags: ["flash_attention", "motif_discovery", "gnn", "ppi"]
  
  # Ablation settings (for experiments)
  ablation:
    use_motifs: true
    use_complementarity: true
    use_gnn: true
    use_properties: true
    use_secondary_structure: true